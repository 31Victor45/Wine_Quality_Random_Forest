{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a13d151",
   "metadata": {},
   "source": [
    "### 3. Aprendizaje en Conjunto (Ensemble Learning)\n",
    "\n",
    "Este es el concepto que da vida a los Bosques Aleatorios. La idea central es combinar múltiples modelos simples y \"débiles\" para crear un solo modelo mucho más robusto y preciso. Es como reunir un comité de expertos que vota para tomar una decisión final, en lugar de confiar en la opinión de un solo individuo.\n",
    "\n",
    "#### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "El **Bagging** es la técnica específica que utilizan los Bosques Aleatorios. Funciona con dos conceptos clave:\n",
    "\n",
    "* **Bootstrapping:** Se crean múltiples subconjuntos de datos, llamados \"muestras bootstrap\", seleccionando de forma aleatoria los datos del conjunto de entrenamiento original. La clave es que la selección se hace **con reemplazo**, lo que significa que una misma fila de datos puede aparecer varias veces en la misma submuestra. Cada uno de estos subconjuntos se utiliza para entrenar un modelo (en este caso, un árbol de decisión) de manera independiente.\n",
    "* **Agregación:** Una vez que cada modelo ha hecho su predicción, los resultados se combinan para llegar a una predicción final. En un problema de clasificación (como el de los vinos), esto se logra mediante una **votación por mayoría**. Si la mayoría de los árboles predice \"alta calidad\", esa será la predicción final. En un problema de regresión, se promedian los resultados de todos los árboles.\n",
    "\n",
    "---\n",
    "### 4. Bosques Aleatorios (el algoritmo en sí)\n",
    "\n",
    "Ahora, combina todo lo anterior para entender el algoritmo completo. Un **Bosque Aleatorio** es una técnica de **Bagging** que agrega una capa adicional de aleatoriedad. En lugar de simplemente promediar los resultados de varios árboles, el algoritmo se asegura de que cada árbol sea único para reducir el sobreajuste y mejorar la precisión.\n",
    "\n",
    "#### ¿Cómo Funciona?\n",
    "\n",
    "El proceso se puede resumir en los siguientes pasos:\n",
    "\n",
    "1.  **Bootstrapping:** Se eligen **$N$** muestras aleatorias del conjunto de datos original **con reemplazo**. Esto significa que algunas filas pueden aparecer varias veces, mientras que otras podrían no ser seleccionadas en absoluto para una muestra específica.\n",
    "2.  **Selección Aleatoria de Características:** Para cada uno de los **$N$** árboles, y en cada división de un nodo, el algoritmo no considera todas las características disponibles, sino solo un subconjunto aleatorio de ellas (definido por el parámetro `max_features`).\n",
    "3.  **Entrenamiento:** Cada árbol se entrena de forma independiente con su subconjunto de datos y características. Esto asegura que cada árbol sea diferente de los demás.\n",
    "4.  **Agregación (Votación):** Para hacer una nueva predicción, cada árbol del bosque \"vota\" su resultado. La predicción final es la que recibe la mayoría de los votos.\n",
    "\n",
    "#### Aleatoriedad\n",
    "\n",
    "La clave del éxito del \"bosque aleatorio\" es la **doble aleatoriedad**:\n",
    "1.  Aleatoriedad en los datos (Bootstrapping).\n",
    "2.  Aleatoriedad en las características (`max_features`).\n",
    "\n",
    "Esta combinación evita que el modelo se sobreajuste a los datos de entrenamiento y lo hace mucho más robusto.\n",
    "\n",
    "#### Ventajas y Desventajas\n",
    "\n",
    "**Ventajas:**\n",
    "* **Alta Precisión:** Es uno de los algoritmos más precisos y robustos para una amplia gama de problemas.\n",
    "* **Manejo de Datos Faltantes:** Puede manejar datos faltantes y desbalanceados de forma efectiva.\n",
    "* **Pocas Suposiciones:** No requiere muchas suposiciones sobre la distribución de los datos.\n",
    "* **Importancia de Características:** Permite identificar fácilmente cuáles características son más importantes para la predicción.\n",
    "\n",
    "**Desventajas:**\n",
    "* **Modelo de \"Caja Negra\":** Es difícil interpretar cómo el modelo llega a una predicción, ya que se basa en el voto de cientos de árboles.\n",
    "* **Costo Computacional:** Requiere más tiempo y recursos computacionales que un solo árbol de decisión.\n",
    "\n",
    "---\n",
    "### Ejemplo de Bosque Aleatorio en Python\n",
    "\n",
    "El siguiente código te muestra cómo crear un `RandomForestClassifier` y su rendimiento. Observa cómo la precisión del bosque es superior a la de un solo árbol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d7f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Precisión del modelo de Bagging con 100 árboles: 0.95\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Precisión de un solo Árbol de Decisión: 0.91\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creamos un dataset de ejemplo\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=5,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Dividimos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Creamos el modelo base: un Árbol de Decisión\n",
    "base_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# 2. Creamos el modelo de Bagging con 100 árboles\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=base_tree, # El modelo base que se replicará\n",
    "    n_estimators=100, # El número de árboles a crear\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Usa todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Entrenamos y evaluamos el modelo\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(\"---\" * 15)\n",
    "print(f\"Precisión del modelo de Bagging con 100 árboles: {accuracy_bagging:.2f}\")\n",
    "print(\"---\" * 15)\n",
    "\n",
    "# A modo de comparación, entrenamos y evaluamos un solo árbol\n",
    "base_tree.fit(X_train, y_train)\n",
    "y_pred_single_tree = base_tree.predict(X_test)\n",
    "accuracy_single_tree = accuracy_score(y_test, y_pred_single_tree)\n",
    "\n",
    "print(\"---\" * 15)\n",
    "print(f\"Precisión de un solo Árbol de Decisión: {accuracy_single_tree:.2f}\")\n",
    "print(\"---\" * 15)\n",
    "\n",
    "# Como puedes ver, el modelo de Bagging (similar al Bosque Aleatorio)\n",
    "# casi siempre tiene una precisión superior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
